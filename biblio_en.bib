@misc{kwon2020pomo,
      title={POMO: Policy Optimization with Multiple Optima for Reinforcement Learning}, 
      author={Yeong-Dae Kwon and Jinho Choo and Byoungjip Kim and Iljoo Yoon and Seungjai Min and Youngjune Gwon},
      year={2020},
      eprint={2010.16011},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhu2021transfer,
      title={Transfer Learning in Deep Reinforcement Learning: A Survey}, 
      author={Zhuangdi Zhu and Kaixiang Lin and Jiayu Zhou},
      year={2021},
      eprint={2009.07888},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@ARTICLE{8743390,
  author={Y. {Sun} and M. {Peng} and Y. {Zhou} and Y. {Huang} and S. {Mao}},
  journal={IEEE Communications Surveys   Tutorials}, 
  title={Application of Machine Learning in Wireless Networks: Key Techniques and Open Issues}, 
  year={2019},
  volume={21},
  number={4},
  pages={3072-3108},
  doi={10.1109/COMST.2019.2924243}}
 
 @ARTICLE{7080987,
  author={M. {Abu Alsheikh} and D. T. {Hoang} and D. {Niyato} and H. {Tan} and S. {Lin}},
  journal={IEEE Communications Surveys   Tutorials}, 
  title={Markov Decision Processes With Applications in Wireless Sensor Networks: A Survey}, 
  year={2015},
  volume={17},
  number={3},
  pages={1239-1267},
  doi={10.1109/COMST.2015.2420686}} 
 
@Book{Sutton+Barto:1998,
  author =       "Sutton, Richard S. and Barto, Andrew G.",
  title =        "Reinforcement Learning: An Introduction",
  publisher =    "MIT Press",
  year =         "1998",
  ISBN =         "0-262-19398-1",
  address =   "Cambridge, MA, USA",
  url = "http://www.cs.ualberta.ca/%7Esutton/book/ebook/the-book.html",
  bib2html_rescat = "Function Approximation, Partial Observability, Learning Methods, General RL, Applications",
}


@ARTICLE{6336689,  author={M. {Bkassiny} and Y. {Li} and S. K. {Jayaweera}},  journal={IEEE Communications Surveys   Tutorials},   title={A Survey on Machine-Learning Techniques in Cognitive Radios},   year={2013},  volume={15},  number={3},  pages={1136-1159},  doi={10.1109/SURV.2012.100412.00017}}

@ARTICLE{WichmanWirelessChallengesandOp,
  author={A. {Sabharwal} and P. {Schniter} and D. {Guo} and D. W. {Bliss} and S. {Rangarajan} and R. {Wichman}},
  journal={IEEE Journal on Selected Areas in Communications}, 
  title={In-Band Full-Duplex Wireless: Challenges and Opportunities}, 
  year={2014},
  volume={32},
  number={9},
  pages={1637-1652},
  doi={10.1109/JSAC.2014.2330193}}

@ARTICLE{liu_xu_chen_wang_wang_li_li_xu_2019,
  author={S. {Liu} and Y. {Xu} and X. {Chen} and X. {Wang} and M. {Wang} and W. {Li} and Y. {Li} and Y. {Xu}},
  journal={IEEE Access}, 
  title={Pattern-Aware Intelligent Anti-Jamming Communication: A Sequential Deep Reinforcement Learning Approach}, 
  year={2019},
  volume={7},
  number={},
  pages={169204-169216},
  doi={10.1109/ACCESS.2019.2954531}}

@article{mnih2015humanlevel,
  added-at = {2015-08-26T14:46:40.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2fb15f4471c81dc2b9edf2304cb2f7083/hotho},
  description = {Human-level control through deep reinforcement learning - nature14236.pdf},
  interhash = {eac59980357d99db87b341b61ef6645f},
  intrahash = {fb15f4471c81dc2b9edf2304cb2f7083},
  issn = {00280836},
  journal = {Nature},
  keywords = {deep learning toread},
  month = feb,
  number = 7540,
  pages = {529--533},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp = {2015-08-26T14:46:40.000+0200},
  title = {Human-level control through deep reinforcement learning},
  url = {http://dx.doi.org/10.1038/nature14236},
  volume = 518,
  year = 2015
} 
 	
 @misc{vanhuynh2020deepfake,
      title={DeepFake: Deep Dueling-based Deception Strategy to Defeat Reactive Jammers}, 
      author={Nguyen Van Huynh and Dinh Thai Hoang and Diep N. Nguyen and Eryk Dutkiewicz},
      year={2020},
      eprint={2005.07034},
      archivePrefix={arXiv},
      primaryClass={cs.NI}
}

@INPROCEEDINGS{ICC:LietWangAntiJamUltraDenseNetwork,
  author={W. {Li} and J. {Wang} and L. {Li} and G. {Zhang} and Z. {Dang} and S. {Li}},
  booktitle={ICC 2019 - 2019 IEEE International Conference on Communications (ICC)}, 
  title={Intelligent Anti-Jamming Communication with Continuous Action Decision for Ultra-Dense Network}, 
  year={2019},
  volume={},
  number={},
  pages={1-7},
  doi={10.1109/ICC.2019.8761578}}
  
@inproceedings{ACM:HasseltetSilver,
  author        = "Hasselt H. and Guez A. and Silver D.",
  title         = "Deep Reinforcement Learning with Double Q-Learning",
  booktitle     = "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence",
  year          = 2016,
  publisher = {AAAI Press},
  numpages = {7},
  address = {Phoenix, Arizona},
  series = {AAAI'16},
}
 
@article{CCRN_Dastangoo_Fossa,
  author={S. {Dastangoo} and C. E. {Fossa} and Y. L. {Gwon} and H. {Kung}},
  journal={IEEE Transactions on Cognitive Communications and Networking}, 
  title={Competing Cognitive Resilient Networks}, 
  year={2016},
  volume={2},
  number={1},
  pages={95-109},
  doi={10.1109/TCCN.2016.2570798}
 }
 

@inproceedings{Kasturi2020MachineLR,
  title={Machine Learning-Based RF Jamming Classification Techniques in Wireless Ad Hoc Networks},
  author={G. Kasturi and Ansh Jain and Jagdeep Singh},
  year={2020}
}

@INPROCEEDINGS{PoorEtHan,  
author={G. {Han} and L. {Xiao} and H. V. {Poor}},  
booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},   title={Two-dimensional anti-jamming communication based on deep reinforcement learning},   year={2017},  
    volume={},  
    number={},  
    pages={2087-2091},  
    abstract={In this paper, a two-dimensional anti-jamming communication scheme for cognitive radio networks is developed, in which a secondary user (SU) exploits both spread spectrum and user mobility to address jamming attacks, while not interfering with primary users. By applying a deep Q-network algorithm, this scheme determines whether to recommend that the SU leave an area of heavy jamming and chooses a frequency hopping pattern to defeat smart jammers. Without knowing the jamming model and the radio channel model, the SU derives an optimal anti-jamming communication policy using Q-learning in a proposed dynamic game, and applies a deep convolution neural network to accelerate the learning speed with a large number of frequency channels. The proposed scheme can increase the signal-to-interference-plus-noise ratio and improve the utility of the SU against cooperative jamming, compared with a Q-learning-only based benchmark system.},  keywords={cognitive radio;convolution;game theory;jamming;learning (artificial intelligence);neural nets;radio networks;spread spectrum communication;telecommunication computing;2D antijamming communication;deep reinforcement learning;cognitive radio networks;spread spectrum;user mobility;jamming attacks;deep Q-network algorithm;frequency hopping pattern;Q-learning;dynamic game;deep convolution neural network;frequency channels;signal-to-interference-plus-noise ratio;cooperative jamming;Jamming;Games;Signal to noise ratio;Interference;Spread spectrum communication;Learning (artificial intelligence);Cognitive radio networks;jamming;deep reinforcement learning;game theory;deep Q-networks},  doi={10.1109/ICASSP.2017.7952524},  ISSN={2379-190X},  month={March},}

@INPROCEEDINGS{FossaEtGwonComMobNetGame,
  author={Y. {Gwon} and S. {Dastangoo} and C. {Fossa} and H. T. {Kung}},
  booktitle={2013 IEEE Conference on Communications and Network Security (CNS)}, 
  title={Competing Mobile Network Game: Embracing antijamming and jamming strategies with reinforcement learning}, 
  year={2013},
  volume={},
  number={},
  pages={28-36},
  abstract={We introduce Competing Mobile Network Game (CMNG), a stochastic game played by cognitive radio networks that compete for dominating an open spectrum access. Differentiated from existing approaches, we incorporate both communicator and jamming nodes to form a network for friendly coalition, integrate antijamming and jamming subgames into a stochastic framework, and apply Q-learning techniques to solve for an optimal channel access strategy. We empirically evaluate our Q-learning based strategies and find that Minimax-Q learning is more suitable for an aggressive environment than Nash-Q while Friend-or-foe Q-learning can provide the best solution under distributed mobile ad hoc networking scenarios in which the centralized control can hardly be available.},
  keywords={cognitive radio;interference suppression;jamming;learning (artificial intelligence);minimax techniques;mobile ad hoc networks;radio access networks;radio spectrum management;stochastic games;telecommunication computing;wireless channels;jamming subgame strategy;antijamming subgame strategy;reinforcement learning;competing mobile network game;CMNG;stochastic game;cognitive radio network;open spectrum access;optimal channel access strategy;minimax-Q learning technique;Nash-Q learning technique;friend-or-foe Q-learning technique;mobile ad hoc network;centralized control;Jamming;Radio frequency;Games;Mobile computing;Mobile communication;Cognitive radio;Aerospace electronics},
  doi={10.1109/CNS.2013.6682689},
  ISSN={},
  month={Oct},
  }
  









 